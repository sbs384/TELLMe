import os
import random
import logging
import torch
import torch.nn.functional as F
import numpy as np
from sklearn.decomposition import PCA
from sklearn.covariance import LedoitWolf


class NumpyWhitening():
    def __init__(self):
        pass
    
    def fit(self, sentence_embeddings):
        self.mu = sentence_embeddings.mean(axis=0, keepdims=True).astype(np.float32)
        # cov = np.cov(sentence_embeddings.T)
        cov = LedoitWolf(assume_centered=True).fit(sentence_embeddings).covariance_
        u, s, vh = np.linalg.svd(cov)
        self.W = np.dot(u, np.diag(1 / np.sqrt(s))).astype(np.float32)
        self.inverse_W = np.dot(np.diag(np.sqrt(s)), u.T).astype(np.float32)
    
    def transform(self, vecs):
        return (vecs - self.mu).dot(self.W)

    def inverse_transform(self, vecs):
        return vecs.dot(self.inverse_W) + self.mu



def whitening(src_embeddings, tgt_embeddings):
    pca_model = PCA(n_components=src_embeddings.shape[1], whiten=True)\
        .fit(np.concatenate([src_embeddings, tgt_embeddings], 0))

    return pca_model.transform(src_embeddings), pca_model.transform(tgt_embeddings)

# def whitening(src_embeddings, tgt_embeddings):
#     whitening_model = NumpyWhitening()
#     whitening_model.fit(np.concatenate([src_embeddings, tgt_embeddings], 0))

#     return whitening_model.transform(src_embeddings), whitening_model.transform(tgt_embeddings)



def random_candidate_sampling(src_embeddings, tgt_embeddings, src_ids, args):

    random.seed(args.seed)
    src_size = src_embeddings.shape[0]
    candidate_size = int(args.candidate_size)
    
    # Load data features from cache or datas et file
    cached_dir = f"./cached_data/{args.dataset}"
    if not os.path.exists(cached_dir):
        os.makedirs(cached_dir)
    cached_file = os.path.join(cached_dir, "random_candidates")
    # load processed dataset or process the original dataset
    if os.path.exists(cached_file):
        logging.info("Loading random candidates from cached file %s", cached_file)
        random_candidates_dict = torch.load(cached_file)
    else:
        random_candidates_dict = dict()
    
    candidate_key = f"{args.seed}_{candidate_size}"
    if candidate_key in random_candidates_dict:
        sampled_candidate_ids = random_candidates_dict[candidate_key]
    else:
        
        equal_matrix = (src_ids[:, np.newaxis].repeat(src_size, 1) == src_ids[np.newaxis, :].repeat(src_size, 0)).astype(np.float32)
        neg_coords = np.where(equal_matrix==0)
        
        neg_ids = []
        cur_row = -1
        sampled_candidate_ids = []
        neg_size = candidate_size - 1
        num_sampling = 0
        for i in range(neg_coords[0].shape[0]):
            if neg_coords[0][i] != cur_row:
                cur_row = neg_coords[0][i]
                if len(neg_ids) > 0:
                    sampled_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
                    num_sampling += 1
                neg_ids.append([])
                
            neg_ids[-1].append(neg_coords[1][i])
        sampled_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))

        random_candidates_dict[candidate_key] = sampled_candidate_ids
        logging.info("Saving candidate ids to %s", cached_file)
        torch.save(random_candidates_dict, cached_file)
    
    sampled_candidate_embeddings = tgt_embeddings[sampled_candidate_ids].reshape(src_size, candidate_size, -1)


    return src_embeddings[:, np.newaxis, :].repeat(candidate_size, 1), sampled_candidate_embeddings


def random_intra_mixup_candidate_sampling(src_embeddings, tgt_embeddings, src_ids, candidate_size, mode, args):
    random.seed(args.seed)
    src_size, embed_size = src_embeddings.shape
    neg_size = candidate_size - 1
    
    # Load data features from cache or datas et file
    cached_dir = f"./cached_data/{args.dataset}"
    if not os.path.exists(cached_dir):
        os.makedirs(cached_dir)
    cached_file = os.path.join(cached_dir, "random_intra_mixup_candidates")
    # load processed dataset or process the original dataset
    if os.path.exists(cached_file):
        logging.info("Loading random candidates from cached file %s", cached_file)
        candidates_dict = torch.load(cached_file)
    else:
        candidates_dict = dict()
    
    candidate_key = f"{args.seed}_{candidate_size}"
    if candidate_key in candidates_dict:
        sampled_qa_candidate_ids, sampled_aq_candidate_ids = candidates_dict[candidate_key]
    else:
        equal_matrix = (src_ids[:, np.newaxis].repeat(src_size, 1) == src_ids[np.newaxis, :].repeat(src_size, 0)).astype(np.float32)
        neg_coords = np.where(equal_matrix==0)
        
        neg_ids = []
        cur_row = -1
        sampled_qa_candidate_ids = []
        # sampled_qq_candidate_ids = []
        sampled_aq_candidate_ids = []
        # sampled_aa_candidate_ids = []

        num_sampling = 0
        for i in range(neg_coords[0].shape[0]):
            if neg_coords[0][i] != cur_row:
                cur_row = neg_coords[0][i]
                if len(neg_ids) > 0:
                    sampled_qa_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
                    # sampled_qq_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
                    sampled_aq_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
                    # sampled_aa_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
                    num_sampling += 1
                neg_ids.append([])
            neg_ids[-1].append(neg_coords[1][i])

        sampled_qa_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
        # sampled_qq_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
        sampled_aq_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))
        # sampled_aa_candidate_ids.extend([num_sampling] + random.sample(neg_ids[-1], neg_size))

        candidates_dict[candidate_key] = (sampled_qa_candidate_ids, 
                                        #   sampled_qq_candidate_ids,
                                          sampled_aq_candidate_ids, 
                                        #   sampled_aa_candidate_ids
                                          )
        logging.info("Saving candidate ids to %s", cached_file)
        torch.save(candidates_dict, cached_file)
    
    sampled_qa_candidate_embeddings = tgt_embeddings[sampled_qa_candidate_ids].reshape(src_size, candidate_size, -1)
    # sampled_qq_candidate_embeddings = src_embeddings[sampled_qq_candidate_ids].reshape(src_size, candidate_size, -1)
    sampled_aq_candidate_embeddings = src_embeddings[sampled_aq_candidate_ids].reshape(src_size, candidate_size, -1)
    # sampled_aa_candidate_embeddings = tgt_embeddings[sampled_aa_candidate_ids].reshape(src_size, candidate_size, -1)

    # [batch_size, candidate_size, hidden_size]
    src_embeddings = src_embeddings[:, np.newaxis, :].repeat(candidate_size, 1)
    tgt_embeddings = tgt_embeddings[:, np.newaxis, :].repeat(candidate_size, 1)

    # [batch_size, candidate_size, 1]
    mixup_weight = np.linspace(1, 0, candidate_size)[np.newaxis, :, np.newaxis].repeat(src_size, 0)
    sampled_qa_candidate_embeddings = mixup_weight * tgt_embeddings + (1- mixup_weight) * sampled_qa_candidate_embeddings
    # sampled_qq_candidate_embeddings = mixup_weight * src_embeddings + (1- mixup_weight) * sampled_qq_candidate_embeddings
    sampled_aq_candidate_embeddings = mixup_weight * src_embeddings + (1- mixup_weight) * sampled_aq_candidate_embeddings
    # sampled_aa_candidate_embeddings = mixup_weight * tgt_embeddings + (1- mixup_weight) * sampled_aa_candidate_embeddings

    if mode == "uni":
        src_list = [src_embeddings]
        candidate_list = [sampled_qa_candidate_embeddings]
    elif mode == "bi":
        src_list = [src_embeddings, tgt_embeddings]
        candidate_list = [sampled_qa_candidate_embeddings, \
                        sampled_aq_candidate_embeddings]
    src_embeddings = np.concatenate(src_list, 0)
    sampled_candidate_embeddings = np.concatenate(candidate_list, 0)
    labels = np.concatenate([np.squeeze(mixup_weight, 2).reshape(-1)] * len(candidate_list), 0)

    return src_embeddings.reshape(-1, embed_size), sampled_candidate_embeddings.reshape(-1, embed_size), labels


def random_mixup_candidate_sampling(src_embeddings, tgt_embeddings, src_ids, candidate_size, seed):
    random.seed(seed)
    # qa
    src_size, embed_size = src_embeddings.shape
    equal_matrix = (src_ids[:, np.newaxis].repeat(src_size, 1) == src_ids[np.newaxis, :].repeat(src_size, 0)).astype(np.float32)
    neg_coords = np.where(equal_matrix==0)
    neg_ids = []
    cur_row = -1
    for i in range(neg_coords[0].shape[0]):
        if neg_coords[0][i] != cur_row:
            cur_row = neg_coords[0][i]
            neg_ids.append([])
        neg_ids[-1].append(neg_coords[1][i])
    
    neg_size = candidate_size - 1
    sampled_qa_candidate_ids = []
    sampled_qa_candidate_embeddings = []
    
    for i in range(src_ids.shape[0]):
        sampled_qa_candidate_ids.append([i] + random.sample(neg_ids[i], neg_size))
        qa_candidate_embeddings = [tgt_embeddings[j: j+1] for j in sampled_qa_candidate_ids[-1]]
        sampled_qa_candidate_embeddings.append(np.concatenate(qa_candidate_embeddings, 0)[np.newaxis, :, :])
    
    sampled_qa_candidate_embeddings = np.concatenate(sampled_qa_candidate_embeddings, 0)

    # [batch_size, candidate_size, hidden_size]
    src_embeddings = src_embeddings[:, np.newaxis, :].repeat(candidate_size, 1)
    tgt_embeddings = tgt_embeddings[:, np.newaxis, :].repeat(candidate_size, 1)

    # [batch_size, candidate_size, 1]
    qa_mixup_weight = np.linspace(1, 0, candidate_size)[np.newaxis, :, np.newaxis].repeat(src_size, 0)
    sampled_qa_candidate_embeddings = qa_mixup_weight * tgt_embeddings + (1- qa_mixup_weight) * sampled_qa_candidate_embeddings

    sampled_candidate_embeddings = sampled_qa_candidate_embeddings

    return src_embeddings, sampled_candidate_embeddings


# def intra_candidate_sampling(src_embeddings, tgt_embeddings, src_ids, candidate_size, seed, args):
#     cached_dir = f"./cached_data/{args.dataset}"
#     plm_name = "sentence-transformers/all-mpnet-base-v2"
#     args.model_name_or_path = plm_name
#     cached_dataset_embedding_file = os.path.join(cached_dir, f'{plm_name}_nn_indices')
#     # load processed dataset or process the original dataset
#     if os.path.exists(cached_dataset_embedding_file):
#         logging.info("Loading encoded dataset from cached file %s", cached_dataset_embedding_file)
#         data_dict = torch.load(cached_dataset_embedding_file)
#         qq_topk = data_dict["qq_topk"]
#         aa_logits = data_dict["aa_logits"]
#     else:
#         # load dataset
#         from train_reqa import prepare_dataloaders
#         train_data_loader, test_question_data_loader, test_candidate_data_loader = prepare_dataloaders(args)
#         # preparing model
#         from models.dual_encoder import RankModel
#         model = RankModel(args)
#         model.to(args.device)
#         from model_selection import obtain_train_embeddings
#         train_question_embeddings, train_answer_embeddings, train_question_ids = obtain_train_embeddings(model, train_data_loader)

#         train_question_embeddings = F.normalize(train_question_embeddings, dim=-1)
#         train_answer_embeddings = F.normalize(train_answer_embeddings, dim=-1)
#         qq_logits = train_question_embeddings.mm(train_question_embeddings.t())
#         aa_logits = train_answer_embeddings.mm(train_answer_embeddings.t())
#         batch_size = train_question_embeddings.shape[0]
#         logit_mask = (train_question_ids.unsqueeze(1).repeat(1, batch_size) == train_question_ids.unsqueeze(0).repeat(batch_size, 1)).float()
#         qq_logits -= logit_mask * 100000000
#         aa_logits -= logit_mask * 100000000

#         qq_topk = torch.topk(qq_logits, k=1).indices.cpu()
#         aa_topk = torch.topk(qq_logits, k=1).indices.cpu()

#         saved_data = {
#             "qq_topk": qq_topk,
#             "aa_topk": aa_topk
#         }
                
#         logging.info("Saving encoded dataset to %s", cached_dataset_embedding_file)
#         torch.save(saved_data, cached_dataset_embedding_file)
#     return train_question_embeddings, train_answer_embeddings, train_question_ids, test_question_embeddings, test_candidate_embeddings, test_ground_truths

